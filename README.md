# chinese_tokenizer中文分词
中文分词是中文文本处理的基础步骤，其重要性不言而喻。  
因为中文句子中没有明显的词界限，这与英文存在显著的区别。因此，当我们进行中文自然语言处理时，首先需要进行精准的分词。  
分词效果的好坏将直接影响到后续词性标注、命名实体识别等模块的性能。  

## 关于本项目内容
本项目结合工业场景里常见的分词需求：  
1.先使用CRF模型进行初步分词  
2.然后利用双向长短期记忆网络-条件随机场（BiLSTM-CRF）模型提升分词精度  
3.搭建分词系统  
4.最终实现了一个工业级别的中文分词工具，能够有效满足工业场景中常见的分词需求  
